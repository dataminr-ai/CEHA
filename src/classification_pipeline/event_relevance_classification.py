import argparse
import random
import time
from textwrap import dedent

import openai
from tqdm import tqdm

from ..utils.evaluation import event_type_scorer
from ..utils.llm_backbone import MistralAiLLMCaller, OpenAILLMCaller
from ..utils.prompts import (
    databricks_llm_prompt,
    databricks_llm_prompt_chat,
    format_prompt,
    generate_few_shot_prompt_list,
    system_prompt,
)
from ..utils.utils import extract_between_tags, load_data


def predict_event_relevance(args, df_train, df_test):
    """
    Predicts the relevance of events in the given test dataset using a specified 
    language model (LLM) and zero-shot/few-shot learning if specified.

    Args:
        args: 
            - llm_name (str, default="gpt4"): Name of the LLM to use ("mistral", "gpt4"). 
            - max_tokens (int, default=512): Maximum number of tokens for LLM responses.
            - temperature (float, default=0.0): Sampling temperature for LLM.
            - few_shot_num (int, default=0): Number of few-shot examples to include in the prompts.
            - openai_api_key (str, default=None): API key for OpenAI (if using GPT models).
            - mistralai_api_key (str, default=None): API key for Mistral.ai (if using mistral models).
            - mistralai_rps (float, default=0): Request per second limit for Mistral.ai (if using mistral models).

        df_train (pd.DataFrame): 
            A DataFrame containing training data with event descriptions and their relevance labels. 
            Used for generating few-shot examples if `few_shot_num > 0`.

        df_test (pd.DataFrame): 
            A DataFrame containing test data with event descriptions. 

    Returns:
        tuple:
            - all_gold_labels (list): A list of ground truth relevance labels ("Yes" or "No")
              for the test dataset, if the "Is the event relevant?" column is present.
              Otherwise, this list will be empty.
            - all_sys_labels (list): A list of predicted relevance labels ("Yes" or "No")
              generated by the LLM for the test dataset.

    Notes:
        - This function uses different LLM backends based on the specified `llm_name`. Please refer to the instruction to set up correct access.
        - The relevance labels are expected to be in the format "Yes" or "No".   
    """
    model_id_dic = {
        "gpt4": "gpt-4o",
        "mistral": "mistral-large-latest",
    }

    # randomly select few-shot examples
    random.seed(42)
    if args.few_shot_num > 0:
        df_train_pos = df_train[df_train["Is the event relevant?_DM"] == "Yes"]
        df_train_neg = df_train[df_train["Is the event relevant?_DM"] != "Yes"]

        few_shot_examples = {"pos": [], "neg": []}
        random_train_pos = random.sample(range(0, len(df_train_pos)), args.few_shot_num)
        random_train_neg = random.sample(range(0, len(df_train_neg)), args.few_shot_num)
        few_shot_examples["pos"] = df_train_pos.iloc[random_train_pos]
        few_shot_examples["neg"] = df_train_neg.iloc[random_train_neg]

    if args.llm_name == "mistral":
        # Define LLM
        llm_caller = MistralAiLLMCaller(mistralai_api_key=args.mistralai_api_key, model_name=model_id_dic["mistral"], timeout=180)
    elif args.llm_name == "gpt4":
        # Define LLM
        llm_caller = OpenAILLMCaller(openai_api_key=args.openai_api_key, model_name=model_id_dic["gpt4"], timeout=180)

    # Define sampling parameters
    sampling_params = {
        "max_tokens": args.max_tokens,
        "temperature": args.temperature,
    }

    # evaluation
    all_gold_labels = []
    all_sys_labels = []
    evaluation_flag = True if "Is the event relevant?_DM" in df_test else False


    for i in tqdm(df_test.to_dict("records")):
        i["llm_answer"] = None
        if args.few_shot_num > 0:
            i["prompt"], pos_examples, neg_examples = generate_few_shot_prompt_list(
                i["Event Description"], i["Country"], few_shot_examples
            )
            input_prompt = databricks_llm_prompt_chat(
                dedent(system_prompt).strip(),
                dedent(i["prompt"]).strip(),
                pos_examples,
                neg_examples,
            )
        else:
            pos_examples, neg_examples = [], []
            i["prompt"] = format_prompt(i["Event Description"], i["Country"])
            input_prompt = databricks_llm_prompt(
                dedent(system_prompt).strip(), dedent(i["prompt"]).strip()
            )
        # Wait for some time between requests for Mistral.ai to avoid rate limit
        if args.llm_name == "mistral" and args.mistralai_rps:
            time.sleep(1/args.mistralai_rps)
        i["llm_answer"] = llm_caller(input_prompt, sampling_params)[0].strip()

        assert i["llm_answer"] is not None
        i["llm_answer_parsed_relevance"] = extract_between_tags(
            "answer", i["llm_answer"]
        )[0]
        
        if evaluation_flag:
            gold_label = "Yes" if i["Is the event relevant?_DM"] == "Yes" else "No"
            all_gold_labels.append(gold_label)

        system_label = (
            "Yes" if i["llm_answer_parsed_relevance"].strip().lower() == "yes" else "No"
        )
        all_sys_labels.append(system_label)
    
    return all_gold_labels, all_sys_labels

    

def main():
    parser = argparse.ArgumentParser(description="Event Relevance Model")
    parser.add_argument("--llm_name", type=str, default="gpt4", help="llm name")
    parser.add_argument(
        "--max_tokens", type=int, default=512, help="max tokens for llm"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.0, help="temperature of llm"
    )
    parser.add_argument(
        "--few_shot_num", type=int, default=0, help="number of few shot examples"
    )
    parser.add_argument(
        "--openai_api_key", type=str, default=None, help="openai api key"
    )
    parser.add_augment(
        "--mistralai_api_key", type=str, default=None, help="mistral.ai api key"
    )
    parser.add_augment(
        "--mistralai_rps", type=float, default=0, help="mistral.ai request per second limit"
    )
    args = parser.parse_args()

    # Load Data
    df_train, df_dev, df_test = load_data("../../data/CEHA_dataset.csv")

    # Get LLM Model Prediction
    all_gold_labels, all_sys_labels = predict_event_relevance(args, df_train, df_test)

    # Calculate Scores
    scores = event_type_scorer(all_sys_labels, all_gold_labels)
    print(
        f"precision: {scores['precision'][:-1]}; recall: {scores['recall'][:-1]}; f1: {scores['f1'][:-1]}"
    )


if __name__ == "__main__":
    main()
